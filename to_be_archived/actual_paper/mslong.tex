1. oscs are important
2. times when you measure influence signal in a counter-intuitive way
3. may not be possible to measure at arbitrarily high rate
4. Here we want to understand how the choice of measurement times influences the signal quality in the experiment
Biological oscillators serve various roles essential to the normal functioning of living systems \cite{goldbeter1997biochemical}. While it is clear that high-temporal resolution plays a key role in experimental studies of such systems. 

1. We are particularly interested in power of  cosinor-based rhythm detection with uncertain parameters.
2. Explicit formula you already get a lot of mileage. We expand upon the original work to explain this. Find that for certain regimes, there is effectively nothing to optimize. On the other hand with wide frequency uncertainty, there is something to look at. For instance circadian study with [1hr,24hr] freq window. 
3. To rigorously understand influence, we prove equivalence theorem dramatically simplifying the problem
4. Possible to benchmark designs making use of staggered grid or sequential grid
5. How close one is to the theoretical optimum? Can solve this using convex programming, and we provide open source code
The ingenious experiments which have built this understanding often must overcome ethical, financial, and fundamental challenges. For instance, studies involving human participants may face ethical and financial constraints on the sampling rate. Rhythmic biological processes with multiple timescales may be infeasible to measure at a high enough rate to simultaneously resolve all features. Faced with the design challenges listed above, it is likely unclear if an experimental design will be as informative as possible. In this paper, we focus on an important aspect of this question: the influence of the measurement collection times on the robustness of biological rhythm detection. Our method allows experimentalists to determine how close their design is to a theoretical optimum, and efficiently construct improved designs. 



1. Need to say what is meant by maximally informative. Limit scope to a particular kind of experiment
2. Statistical power is a good option
3. Uniform 
We assume that the primary goal of the study is to reliably detect oscillations of unknown periodicities, and estimate their temporal features (amplitudes and acrophases) using a one-frequency cosinor model. 



We quantify the robustness of the sampling strategy using the statistical power of the cosinor model.  Given that acrophase of a signal may be heterogeneous and is rarely known prior to data collection, we will seek designs that perform well for a variety of acrophases. Using an explicit formula for the statistical power of the one-frequency cosinor model \cite{}, we show that maximizing the power of a study across several acrophases is equivalent to a much simpler optimization problem.
 

Uniform designs -- where data is collected in equally spaced intervals -- are likely the most familiar sampling strategy for rhythm detection. While these designs perform well for  oscillations of low-frequency relative to the sampling rate, we find that there is a threshold at half the sampling frequency (i.e. the Nyquist rate) where the statistical power becomes heavily dependent on the acrophase of the signal. This dependence may induce artificial correlations in the data, and uncontrollable bias in the resulting analysis. 


\section{Methods}
- Cosinor model, power, Fisher information matrix, reduced Fisher information matrix

\subsection{The cosinor model}\label{sec:cosinor}
Harmonic regression, or the ``cosinor model'', is a common statistical framework for studying systems with a dominant frequency \cite{cornelissen2014cosinor}. The single-frequency cosinor model with mesor $Y_0$, amplitude $A$, acrophase $\phi$, and frequency $f$ is given by 
\begin{align}
    Y(t) =Y_0+ A \cos(2\pi f t - \phi) + \veps(t) \label{eqn:cosinorAmp},
\end{align}
in which $\veps \sim N(0,\sigma)$ is homoscedastic Gaussian white noise. Equation ~\eqref{eqn:cosinorAmp} also appears in Fourier regression, with the important difference that the frequency must be an integer multiple of the fundamental frequency \cite{TDOO}. This additional constraint on the frequency simplifies the optimization procedure, and in many cases, uniformly spaced measurements are known to be optimal. For this reason it is important that we are not considering Fourier models.

In applications, the frequency parameter $f$ is often estimated from a periodogram or related spectral estimator prior to fitting Eq.~\eqref{eqn:cosinorAmp}  to data. This allows parameter estimation to be recast as a linear regressino problem. Indeed, assuming the frequency is known,  Eq.~\eqref{eqn:cosinorAmp} can be rewritten as
\begin{align}
    Y(t) = \beta_0 + \beta_1 \cos(2\pi f t ) + \beta_2 \sin(2\pi f t) + \veps(t), \label{eqn:cosinorlin}
\end{align}
in which the unknown parameters $\beta_0,\beta_1, \beta_2 \in \bR$ all appear linearly. Given data $\vb{y}=\{y_i\}_{i=1}^{N}$ measured at distinct times $\{t_i\}_{i=1}^{N} \subset [0,1]^N$, the optimal solution for estimating the coefficients in Eq.~\eqref{eqn:cosinorlin} is 
\begin{align}
    \hat{\beta} = (X^T X)^{-1}X^T \vb{y},\label{eqn:betahat}
\end{align}
in which the matrix $X\in \mathbb{R}^{N\times 3}$ is given by
\begin{align}\label{eqn:matX}
    X := \begin{pmatrix}
        1 & \cos(2\pi f t_1) & \sin(2\pi f t_1) \\
       \vdots  &  \vdots             & \vdots \\
        1 & \cos(2\pi f t_N) & \sin(2\pi f t_N). 
    \end{pmatrix}
\end{align}
The columns of $X$ are linearly independent since the measurement times are asummed to be distinct. Later in our work, we will make use of the Fisher information matrix $M$ and reduced Fisher information matrix $\tilde{M}$ given by
\begin{align}
    M = X^T X , \quad\quad \tilde{M} = \tilde{X}^T \tilde{X}, \label{eqn:FIM}
\end{align}
in which $\tilde{X} := [\cos(2\pi f \vb{t}) \;\; \sin(2\pi f \vb{t})]$. The amplitude and acrophase of the signal can be estimated from $\hat{\beta}$, to obtain
\begin{align}
    \hat{A} &= \sqrt{\hat{\beta}_1^2 + \hat{\beta}_2^2}, \label{eqn:amp}\\
    \hat{\phi} &= \arctan{(\hat{\beta}_2/\hat{\beta}_1)}\label{eqn:acro}.
\end{align}
We set the noise strength $\sigma=1$ to reduce the number of parameters in the model. In these units, the amplitude parameter $A$ can be interpreted as the signal-to-noise ratio. The goal of this work is to understand how the amplitude $A$, frequency $f$, and measurement schedule $\vb{t}=(t_1,\ldots,t_N)$ influence the statistical power of our study through the estimator $\hat{\beta}$.

\subsection{An exact expression for harmonic regression power }
We will use the statistical power to quantify how choices of measurement schedules influence the reliability of oscillation detection at various frequencies, amplitudes, and acrophases. 
\begin{defn}[Statistical power]
  For a binary hypothesis test of size $\alpha$, with a rejection region $R_\alpha=\{x: T(x)>c_\alpha\}$ for a test statistic $T(x)$, and parameters $\theta\in\mathbb{R}^p$, the power function is given by
  \begin{equation}
    \gamma(\theta) :=P(X(\theta) \in R_\alpha)=P(T(X(\theta)) > c_\alpha),
  \end{equation}
in which $P_\theta(X(\theta) \in R_\alpha)$ denotes the probability of the random variable $X(\theta)$ belonging to the rejection region $R_\alpha$ for the parameter set $\theta$.
\end{defn}
A thorough introduction to power analysis is provided in \cite{wassermanTextbook}. The power of a hypothesis test is often estimated through permutation tests \cite{Boos2000-hd}. These tests can are computationally expensive and limit the numerical methods available for power optimization. For instance, the power estimates would be too noisy for directly estimating a gradient, ruling out gradient-based optimizers. We avoid Monte Carlo simulations by assuming that the noise in our model is homoscedastic, justifying the following exact expression for statistical power.
\begin{theorem}[Power of harmonic F test, \cite{wei2023circpower}]
  \label{thm:circapower}
Consider the one-frequency cosinor model applied to data $\{y_i\}_{i=1}^{N}$ collected at times $(t_i)_{i=1}^{N}$, with amplitude $A$, noise level $\sigma$, frequency $f$, acrophase $\phi$, and type I error rate $\alpha\in (0,1)$. Suppose we test the hyoptheses
\begin{align}
    H_0 : \beta_1=0=\beta_2, \quad\quad H_1 : \beta_1\neq0 \textrm{ or } \beta_2 \neq 0 
\end{align}
with $H_0$ as the null and $H_1$ as the alternative, using the test statistic
\begin{equation}
F = \frac{ \frac{\textrm{TSS} - \textrm{RSS}}{r-1} }{ \frac{\textrm{RSS}}{N-r}},
\end{equation}
in which $\textrm{TSS} = || \vb{y} - X\hat{\beta}||_2^2$, $\textrm{RSS} =  ||\vb{y}-\langle\vb{y} \rangle||_2^2$, and $r=3$ is the number of degrees of freedom in the model. Then, the power $\gamma(A,\sigma,f,\phi)$ is given by the expression
\begin{align}
  \gamma(A,\sigma,f,\phi)=1- F_{\lambda(\vb{t};A,\sigma,f,\phi)}\left(F_0^{-1}\left( 1-\alpha;2,N-3\right);2,N-3\right) \label{eqn:cpower}
\end{align}
in which
\begin{align}
\lambda(\vb{t};A,\sigma,f,\phi)=\frac{A^2}{\sigma^2}\sum_{i=1}^{N}  \cos^2(2\pi ft_i - \phi),\label{eqn:lambda}
\end{align}
% TODO explain why we are following different power convention and why acrophase for us is in angular rather than temporal units
$F(x;n_1,n_2)$ is the F-distribution on $(n_1,n_2)$ degrees of freedom, and $F_\lambda(x;n_1,n_2)$ is the non-central F distribution with $(n_1,n_2)$ degrees of freedom with non-centrality parameter $\lambda\geq 0$. For $\lambda=0$, the non-central distribution $F_0$ agrees with the standard F distribution.
\end{theorem}
Notice that the non-centrality parameter $\lambda(\vb{t};A,\sigma,f,\phi)$  summarizes the influence of the measurement schedule on the statistical power in Theorem~\ref{thm:circapower}. A corollary from \cite{wei2023circpower} is also of relevance to our work, since it outlines the limitations of power optimization for harmonic regression.
\begin{corollary}\label{cor:unif}
If a measurement schedule is made up of $N\geq3$ measurements per cycle and the measurements are equally spaced over the course of the cycle, the power function in Eq.~\eqref{eqn:cpower} is independent of the acrophase.
\end{corollary}
Based on the corollary above, it only makes sense to consider alternatives to uniform designs when the sampling rate is low in comparison to the frequencies under investigation. Otherwise the power is only dependent on factors outside the experimentalist's immediate control. 

\subsection{Convex programming for optimal experimental design}\label{sec:oed}
Depending on the objective, it may be possible to treat experimental design as a mathematical optimization problem. Standard choices of design criteria lead to convex programming problems \cite{fedorov1972theory}. For instance, Elfving optimality, or E-optimality for short, is the criterion most relevant to our work. E-optimality can be interpreted geometrically as minimizing the worst case variance of an arbitrary linear combination of the regression coefficient estimators $\hat{\beta}$. 
\begin{defn}[E-optimality]\label{defn:eopt}
    For a linear regression model $\vb{y} = X \beta +\veps$, if the Fisher information matrix $X^T X$ is a sum of rank one symmetric matrices $X^TX = \sum_{i=1}^{M} \mu_{i} \vb{v}_{i} \vb{v}_{i}^T$, then the E-optimality criterion takes the form
    \begin{alignat*}{2}
        &\textrm{minimize}   \quad\quad&&\norm{ \left(\sum_{i=1}^{M} \mu_{i} \vb{v}_{i}\vb{v}_{i}^{T} \right)^{-1} }_2\\
        &\textrm{subject to}  \quad\quad&& \bmu\geq 0,  \quad \vb{1}^T \bmu = 1. 
    \end{alignat*} 
Disciplined convex programming allows one to construct E-optimal designs numerically, even in the presence of integer constraints. E-optimality can also be framed as the following semi-definite program
    \begin{alignat*}{2}
        &\textrm{maximize}   \quad\quad&& s \\
        &\textrm{subject to}  \quad\quad&& \sum_{i=1}^{M} \mu_{i} \vb{v}_{i}\vb{v}_{i}^T \succcurlyeq s I, \quad  s\in \bR, \quad \bmu\geq 0,  \quad \vb{1}^T \bmu = 1.  
    \end{alignat*} 
The notation $A \succcurlyeq B$ means that $A-B$ is symmetric positive definite. 
\end{defn}
Our definition of the reduced Fisher information matrix in Eq.~\eqref{eqn:FIM} agrees with the structure required in E-optimalty, since the reduced Fisher information matrix $\tilde{M}$  can be expressed as
\begin{align}
    \tilde{X}^T \tilde{X} = \sum_{i=1}^{N}   \begin{bmatrix}
        \cos(2\pi f t_i)\\\sin(2\pi f t_i)
    \end{bmatrix}\begin{bmatrix}
        \cos(2\pi f t_i) & \sin(2\pi f t_i)
    \end{bmatrix}.
\end{align}
The basics of semi-definite programming and their applications to optimal experimental design are discussed in detail in \cite{boyd2004convex}. 

\section{Results}\label{sec:results}
\subsection{An optimization framework for single-frequency power analysis}
- We begin by stating the main result and defer its derivation to our methods section found later in article
- Some things are immediate from the result in MedStat article
- A reduction theorem
- Heuristics for simplest non-uniform
- A method for optimization


For our analysis, we assume that the frequency $f$ is known ahead of time for instance from periodogram analysis, and that the experimentalist seeks to maximize the power over all acrophases for a fixed frequency. Under these assumptions, we aim to choose a set of $N$ measurement times $\vb{t}=(t_1,\ldots,t_N)$ such that
\begin{align}
\vb{t}^* &= \underset{\vb{t} \in [0,1]^{N}}{\textrm{argmax}} \underset{\phi \in [0,2\pi)}{\textrm{min}}  \gamma(\vb{t};A,f,\phi)  \label{eqn:mmax}.
\end{align}
Our first contribution is to show that the optimization problem in Eq.~\eqref{eqn:mmax} is equivalent to maximizing the non-centrality parameter $\lambda$ appearing in the non-central F-distribution in Eq.~\eqref{eqn:lambda}.
\begin{prop}
    The non-central F distribution $F_\lambda(x;n_1,n_2)$ with degrees of freedom $n_1=2$ and $n_2=N-3$ where $N>3$ is a monotone decreasing function of the non-centrality parameter $\lambda$. 
\end{prop}

\begin{proof}
The function $F(x;n_1,n_2,\lambda)$ can be expressed as a series
\begin{align}
    F(x;n_1,n_2,\lambda) = \sum_{k=0}^{\infty} \left( \frac{\left(\frac{\lambda}{2}\right)^k}{k!}e^{-\lambda/2} \right) I\left(\frac{n_1 x}{n_2+n_1x};\frac{n_1}{2}+k,\frac{n_2}{2}\right), \quad\quad  \label{eqn:ncfcdf}
\end{align}
with random variable $x\geq 0$, degrees of freedom $n_1$,$n_2 >0$, and non-centrality parameter $\lambda \geq 0$. The incomplete beta function $B(z;a,b)$ and regularized incomplete beta function $I(z;a,b)$ appearing in Eq.~\eqref{eqn:ncfcdf} are defined by
\begin{align}
 B(z;a,b) &:= \int_0^z t^{a-1} (1-t)^{b-1}~\diff t ,  \\
 I(z;a,b) &:= B(z;a,b)/B(1;a,b).
\end{align}
Let $z:= \frac{n_1 x}{n_2 + n_1 x}$ and $I_k := I(z;\frac{n_1}{2}+k,\frac{n_2}{2})$. We may use a recurrence relation for beta functions \cite{stegun64} 
\begin{align}
    I(z;a+1,b) = I(z;a,b) - \frac{z^a(1-z)^b}{aB(1;a,b)},
\end{align}
together with the fact that $z\in[0,1]$ to  conclude  that 
\begin{align}
   I(z;a+1,b) \leq I(z;a,b).
\end{align}
From this inequality it follows that Eq.~\eqref{eqn:ncfcdf} is uniformly convergent in $\lambda$, which permits term-wise differentiation and monotonicity as an immediate consequence. 

To justify term-wise differentiation of a series  $F(x;n_1,n_2,\lambda) = \sum_{k=0}^\infty T_k(z,n_1,n_2,\lambda)$ for any $\lambda\in[0,\Lambda]$ it suffices to show 
\begin{itemize}
    \item $T_k(z,n_1,n_2,\lambda)$ is continuous in $[0,\Lambda]$,
    \item there exists $\lambda^* \in [0,\Lambda]$ such that $\sum_{k=0}^\infty T_k(z,n_1,n_2,\lambda^*)$ converges, and
    \item $\sum_{k=0}^\infty \partial_\lambda T_k(z,n_1,n_2,\lambda)$ converges uniformly for all $\lambda\in[0,\Lambda]$.
\end{itemize}
Clearly the functions $T_k$ are continuous in $[0,\Lambda]$. Convergence follows from applying the ratio test to
\begin{align}
     \frac{T_{k+1}(z,n_1,n_2,\lambda)}{T_k(z,n_1,n_2,\lambda)} = \frac{I_{k+1}}{I_k}  \frac{\lambda}{2(k+1)} \leq \frac{\lambda}{2(k+1)},
\end{align}
which implies $\lim_{k\to\infty} \left|  \frac{T_{k+1}(z,n_1,n_2,\lambda)}{T_k(z,n_1,n_2,\lambda)} \right| =0$, verifying convergence for any $\lambda\in[0,\Lambda]$. 
Uniform convergence of the derivatives can be obtained from the Weierstrass M-test. To construct a majorant for $T_k(z,n_1,n_2,\lambda)$ notice that $\partial_\lambda T_k$  has critical points at $\lambda=2k$ and $\lambda =0$. We know that $T_k(z,n_1,n_2,0) = 0$ and $\lim_{\lambda \to \infty} T_k(z,n_1,n_2,\lambda) = 0$ so 
\begin{align}
 g_k(z,n_1,n_2) := T_k(z,n_1,n_2,2k)   = I_{k} \frac{k^k}{k!} e^{-k}
\end{align}
must be the global maximum of $T_k(z,n_1,n_2,\lambda)$ on the interval $\lambda\in[0,\infty)$. This majorant also leads to a bound on the derivative, since
\begin{align}
    \frac{\partial}{\partial \lambda} T_k(z;n_1,n_2,\lambda) &= - \frac{1}{2}T_k(z;n_1,n_2,\lambda) +  e^{-\lambda/2} \frac{ \lambda^{k-1}}{(k-1)! 2^k} I\left( z; \frac{n_1}{2} +k , \frac{n_2}{2}\right)\\
    &< - \frac{1}{2}T_k(z;n_1,n_2,\lambda) + \frac{1}{2}e^{-\lambda/2} \frac{\lambda^{k-1}}{(k-1)!2^{k-1}}I\left(z;\frac{n_1}{2}+k-1,\frac{n_2}{2}\right)\\
    &= - \frac{1}{2}\left( T_k\left(z;n_1,n_2,\lambda\right)  - T_{k-1}\left(z;n_1,n_2,\lambda\right)  \right),
\end{align}
which implies
\begin{align}
    \left|\frac{\partial}{\partial \lambda} T_k(z;n_1,n_2,\lambda) \right| \leq \frac{1}{2}\left( g_k(z;n_1,n_2) + g_{k-1}(z;n_1,n_2)\right) =: G_k(z;n_1,n_2).
\end{align}
To prove uniform convergence of $\sum_{k} \partial_\lambda T_k$, it remains to show that $\sum_{k=0}^\infty G_k$ converges, which follows from showing that $\limsup_{k\to\infty}  \left| \frac{g_{k+1}}{g_k}\right| <1$. Indeed, we have
\begin{align}
   \frac{g_{k+1}}{g_k} = \frac{I_{k+1}}{I_k}  \frac{(k+1)^{k+1}}{k^k} \frac{k!}{(k+1)!} e^{-1} =\frac{I_{k+1}}{I_k} \left( \frac{k+1}{k}\right)^k e^{-1} =  \frac{I_{k+1}}{I_k} e^{-1}\left(1+\frac{1}{k}\right)^k.
\end{align}
We know $\lim_{k\to\infty} \left( 1 + \frac{1}{k}\right)^k = e $ and using again the recurrence property of beta functions, 
\begin{align}
    \limsup_{k\to\infty}    \left| \frac{g_{k+1}}{g_k}\right|  =  \limsup_{k\to\infty} \frac{I_{k+1}}{I_k} < 1,
\end{align}
which verifies that $\sum_{k=0}^\infty g_k$ converges by the ratio test. Applying the differentiation theorem for uniformly convergent series, we obtain
\begin{align}
        \frac{\partial }{\partial \lambda} F(x;n_1,n_2,\lambda) &= \sum_{k=0}^\infty \frac{\partial}{\partial \lambda} T_k(z;n_1,n_2, \lambda) <\sum_{k=0}^\infty- \frac{1}{2}\left( T_k\left(z;n_1,n_2,\lambda\right)  - T_{k-1}\left(z;n_1,n_2,\lambda\right)  \right)\\
        &= \lim_{N\to\infty} -\frac{1}{2} T_N(z;n_1,n_2,\lambda) \leq 0.
    \end{align}
Since the choice of interval $[0,\Lambda]$ was arbitrary, we conclude that $F(x;n_1,n_2,\lambda)$ is monotone decreasing for all $\lambda \geq 0$.

\end{proof}
Since the power $\gamma(A,f,\phi)$ increases with $\lambda$ by the preceding proposition,  the optimal $\vb{t}^*$ appearing in Eq.~\eqref{eqn:mmax} can be found by simply maximizing the non-centrality parameter, $\lambda$. This avoids working with the non-central F distribution. We take this reduction further by showing that the minimax approach to optimizing $\lambda$ can be reframed as a convex programming problem.
\begin{prop}
   Maximizing the non-centrality parameter $\lambda$ uniformly over acrophase is equivalent to maximizing the smallest singular value of the reduced design matrix $\tilde{X} = [\cos(2\pi f \vb{t})\;\; \sin(2\pi f \vb{t}) ] $ .
\end{prop} 
\begin{proof}
The parameter $\lambda$  can be expressed in terms of the reduced design matrix 
\begin{align}
    \lambda(\vb{t};A,f,\phi) = A^2 \sum_{i=1}^{N} \cos^2(2\pi f t_i -\phi ) =  ||\tilde{X} [\beta_1 \;\; \beta_2]^T ||_2^2. \label{eqn:red}
\end{align}
We reparameterize Eq.~\eqref{eqn:red} to obtain
\begin{align*}
   \max_{\vb{t} \in [0,1]^N} \min_{\phi \in [0,2\pi)}   \lambda(\vb{t};A,f,\phi)  =    \max_{\vb{t} \in [0,1]^N} \min_{\beta_1 ^2 + \beta_2^2 =A^2}  ||\tilde{X} [\beta_1 \;\; \beta_2]^T ||_2^2.
\end{align*}
By homogeneity, we may assume $A^2=1$, reducing our problem to finding 
\begin{align}
    \vb{t}^* = \underset{\vb{t} \in [0,1]^{N}}{\textrm{argmax}} \min_{\beta_1 ^2 + \beta_2^2 =1}  ||\tilde{X} [\beta_1 \;\; \beta_2]^T ||_2^2.\label{eqn:equi1}
\end{align}
The min-max principle for the singular values of a matrix $A \in \mathbb{C}^{n\times n}$ is 
\begin{align}
    \sigma_{i}(A) = \max_{\mathrm{dim}(U)= i } ~ \min_{x\in U, ||x||=1} ||Ax||\label{eqn:equi2}.
\end{align}
Here $\sigma_n \leq \sigma_{n-1} \leq \cdots \sigma_1$, so $i=2$ gives the smallest singular value. When $i=2$, the right hand side of Eq.~\eqref{eqn:equi1} agrees with the square of the right hand side of Eq.~\eqref{eqn:equi2}, verifying the claim.
\end{proof}
If we assume that there are sufficiently many measurements for $\tilde{X}^T\tilde{X}$ to be invertible, then it is clear that $\tilde{X}^T \tilde{X}$ is symmetric positive definite and therefore has real eigenvalues. It follows that we can reduce singular value optimization to a familiar convex optimization problem known as E-optimality.
\begin{corollary}
    If $\tilde{X}$ is non-singular, then 
    \begin{align}
        \sigma_2^2(\tilde{X}) = \lambda_{\mathrm{min}}(\tilde{X}^T \tilde{X}) = \lambda_{\mathrm{max}}((\tilde{X}^T \tilde{X})^{-1}).
    \end{align}
where $\lambda_{\textrm{min}} ( \cdot )$ and $\lambda_{\textrm{max}} ( \cdot )$ refer to the largest and smallest eigenvalues, respetively.
\end{corollary}
Combining the preceding propositions, we have obtained the following equivalence theorem.
\begin{theorem}\label{thm:equiv}
Consider a rhythm detection experiment with measurement budget $N$, amplitude $A$, and frequency $f$. Under the following non-degeneracy conditions
\begin{enumerate}
    \item $N\geq 4$,
    \item $\tilde{X}^T \tilde{X}$ is non-singular,
    \item $F_0^{-1}(1-\alpha;2,N-3) \neq 0$, 
\end{enumerate}
it holds that maximizing power over acrophase in the sense of Eq.~\eqref{eqn:mmax} is equivalent to finding $\vb{t}^*$ such that
\begin{align}
    \vb{t}^* = \argmin_{\vb{t} \in [0,1]^N} \;\; \lambda_{\mathrm{max}} ((\tilde{X}^T \tilde{X})^{-1}).\label{eqn:simp}
\end{align}
\end{theorem}

We propose two methods for solving the optimization problem in Eq.~\eqref{eqn:simp}. The first method is a standard approach to convex optimization problems in optimal experimental design. Instead of having arbitrary choices of measurement times, we fix a discrete grid $\{\tau_{j}\}_{j=1}^{M}$ where $M\gg N$ and assume that we collect data at $N$ of the $M$ times on this grid. Under these assumptions, the Fisher information matrix can be expressed as
\begin{align}
 \tilde{M} = \sum_{j=1}^{M}  \mu_j \begin{bmatrix}
        \cos(2\pi f \tau_j)\\ 
        \sin(2\pi f \tau_j)
    \end{bmatrix}
    \begin{bmatrix} \cos(2\pi f \tau_j) & \sin(2\pi f \tau_j) \end{bmatrix},\label{eqn:disc}
\end{align}
in which the weight function $\{\mu_{j}\}_{j=1}^{M}$ obeys $\mu_{j}\in \{0,1\}$ for $j=1,\ldots, M$ and $ \sum_{j=1}^{M} \mu_{j} = N$. 
The upside of discretizing our choices is that the decision variables $\mu_{j}$ appear linearly in Eq.~\eqref{eqn:disc}. As a consequence, any convex functional of the Fisher information matrix will now be convex in our decision variables. As mentioned in Sec.~\ref{sec:oed}, we may treat this problem as an integer constrained semi-definite program. Such problems can be readily solved by open-source solvers in the JuMP package available in Julia \cite{Lubin2023}.

Our second method is usually faster, but relies on the additional assumption that the Fisher information matrix has distinct eigenvalues. 

Assuming the eigenvalues and eigenvectors are distinct, we may use the fact that for a symmetric and real matrix $A$ with non-degenerate spectrum
\begin{align}
    \nabla_{\vb{t}} \lambda_{i} (A(\vb{t})) = \vb{v}_{i}^T \nabla_{\vb{t}} A(\vb{t}) \vb{v}_i = \begin{bmatrix}
         \vb{v}_{i}^T \partial_{t_{1}}  A(\vb{t}) \vb{v}_i\\
         \vdots \\
         \vb{v}_{i}^T \partial_{t_{N}}  A(\vb{t}) \vb{v}_i
    \end{bmatrix}\label{eqn:grad}
\end{align}
in which $A\vb{v}_{i} = \lambda_i \vb{v}_{i}$, see \cite{horn2012matrix} for a proof.
\subsection{Application to ultradian rhythm detection}

The statistical power corresponding to a uniform becomes phase dependent when the sampling rate is close to an integer multiple of the signal's frequency. The removal of this spurious phase dependence by the methods of the previou section is shown in Fig.~1. The uniform and optimal non-uniform designs for a nominal frequency are compared in Fig.~1a. As expected, the unwanted phase dependence is present in the uniform design but absent from the optimal design in Fig.~1b. Performance remains favourable for the uniform design when a range of frequencies are considered in Fig.1c. 

To better understand the robustness of optimal designs, we investigate their relative performance for a range of frequencies and amplitudes in Fig.~2. 

Fig.~3 deals with effective down-sampling

The linear structure of a one-frequency cosinor model is deeply embedded in our framework. In practice, it is likely that there will be some degree of frequency uncertainty. We investigate the performance of our method in the presence of frequency uncertainty by making use of a weighted objective function. We assume that frequencies are generated from a bimodal distribution and the design under consideration is either uniform, optimized using equally weighted frequencies from a window, or optimized using the true frequency distribution. A comparison of these scenraios is shown in Fig.~4.

\subsection{Derivation of method}

\section{Discussion}
Our main contribution is an efficient numerical method for optimizing the statistical power for signals of all acrophases  for a given amplitude, sampling budget, and frequency. Derivation of our method relies upon an expression for the statistical power of a one-frequency cosinor model under the assumption of homoscedastic Gaussian noise \cite{wei2023circpower}. Using this formula, we avoid Monte Carlo simulations, substantially improving our optimization procedure and its performance.  The reduction to a convex programming problem shows that power optimization is equivalent to maximizing the largest eigenvalue of the inverse Fisher information matrix. The latter problem is known as E-optimality and has been well-studied in the field of optimal experimental design \cite{pukelsheim1993,oed2,oed3}. This connection makes our  optimization method orders of magnitude faster than naive power optimization. 
\begin{itemize}
    \item Theoretical extensions: other ways of talking about power in OED Relation to KL optimality \cite{KLpaper}, weaker assumptions on the residuals. For instance, two moments method \cite{wasserman2022} together with Weingarten calculus could give estimates for this setting. Another relevant question is extension to period uncertainty and bayesian OED \cite{bayesian}. Question of closed-from expressions. Closely related area of Fourier regression \cite{dette1,dette2} has exact expressions, but the starting assumption is different, there you are assuming you have dominant period. Can in fact prove uniform is optimal, important that we are not doing Fourier \cite{fourierEoptimality}. 
    \item Practical extensions: Another relevant idea is aliasing and spectral features \cite{stoica2009}. Using an aliasing heuristic, you can frame anti aliasing as convex problem and integrate this with our method. Application to particular experiments is another interesting idea, how do you do this for different datasets, how much power could you expect to obtain. 
    \item Specific experimental applications
\end{itemize}
